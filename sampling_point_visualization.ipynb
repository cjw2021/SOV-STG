{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelhub import pretrained_sov_stg_s, ImageProcessor, ResultParser, draw_boxes\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some useful function tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def getJetColorRGB(v, vmin, vmax):\n",
    "    c = np.zeros((3))\n",
    "    if (v < vmin):\n",
    "        v = vmin\n",
    "    if (v > vmax):\n",
    "        v = vmax\n",
    "    dv = vmax - vmin\n",
    "    if (v < (vmin + 0.125 * dv)): \n",
    "        c[0] = 256 * (0.5 + (v * 4)) #B: 0.5 ~ 1\n",
    "    elif (v < (vmin + 0.375 * dv)):\n",
    "        c[0] = 255\n",
    "        c[1] = 256 * (v - 0.125) * 4 #G: 0 ~ 1\n",
    "    elif (v < (vmin + 0.625 * dv)):\n",
    "        c[0] = 256 * (-4 * v + 2.5)  #B: 1 ~ 0\n",
    "        c[1] = 255\n",
    "        c[2] = 256 * (4 * (v - 0.375)) #R: 0 ~ 1\n",
    "    elif (v < (vmin + 0.875 * dv)):\n",
    "        c[1] = 256 * (-4 * v + 3.5)  #G: 1 ~ 0\n",
    "        c[2] = 255\n",
    "    else:\n",
    "        c[2] = 256 * (-4 * v + 4.5) #R: 1 ~ 0.5                      \n",
    "    return c\n",
    "\n",
    "def getJetColorRB(v, vmin, vmax):\n",
    "    c = np.zeros((3))\n",
    "    if (v < vmin):\n",
    "        v = vmin\n",
    "    if (v > vmax):\n",
    "        v = vmax\n",
    "    # if (v < (vmin + 0.5 * dv)):\n",
    "    #     c[0] = 256 * (1-(v-vmin)) #B: 0.5 ~ 1\n",
    "    # else:\n",
    "    #     c[2] = 256 * (v-vmin) #R: 1 ~ 0.5\n",
    "    c[0] = 256 * (1-(v-vmin)) #B: 0.5 ~ 1\n",
    "    c[2] = 256 * (v-vmin) #R: 1 ~ 0.5                      \n",
    "    return c  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sov-stg-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'params/sov-stg-s.pth'\n",
    "model, PostPeocessor = pretrained_sov_stg_s(checkpoint_path, True)\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iamge_path = 'data/hico_det/images/test2015/HICO_test2015_00000001.jpg'\n",
    "img, img_size, orig_img = ImageProcessor(iamge_path, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register hooks and forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_value_cross_attn, sampling_offsets, attention_weights, verb_boxes_list = [], [], [], []\n",
    "def hook_cross_attn_input(model, inputs, output):\n",
    "    N, Len_q, _ = inputs[0].shape\n",
    "    hook_value_cross_attn.append([inputs[1], N, Len_q, model.n_heads, model.n_levels, model.n_points, inputs[3]])\n",
    "hooks = [\n",
    "        model.transformer.vDec.layers[-1].cross_attn.register_forward_hook(\n",
    "            hook_cross_attn_input\n",
    "        ),\n",
    "        model.transformer.vDec.layers[-1].cross_attn.sampling_offsets.register_forward_hook(\n",
    "            lambda self, input, output: sampling_offsets.append(output)\n",
    "        ),\n",
    "        model.transformer.vDec.layers[-1].cross_attn.attention_weights.register_forward_hook(\n",
    "            lambda self, input, output: attention_weights.append(output)\n",
    "        ),\n",
    "        model.transformer.vDec.register_forward_hook(\n",
    "            lambda self, input, output: verb_boxes_list.append(output[1][0])\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model([img])\n",
    "results = PostPeocessor(outputs, img_size)\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process hook value\n",
    "reference_points, N_, Len_q, n_heads, n_levels, n_points, value_spatial_shapes = hook_value_cross_attn[0]\n",
    "sampling_offsets_reshape = sampling_offsets[0].view(N_, Len_q, n_heads, n_levels, n_points, 2)\n",
    "sampling_grids = reference_points[:, :, None, :, None, :2] \\\n",
    "                    + sampling_offsets_reshape / n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n",
    "attention_weights_reshape = attention_weights[0].view(N_, Len_q, n_heads, n_levels * n_points)\n",
    "\n",
    "attention_weights_reshape = F.softmax(attention_weights_reshape, -1).view(N_, Len_q, n_heads, n_levels, n_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sampling location to image\n",
    "sampling_grid_rescale_all_lvl = []\n",
    "attention_weights_all_lvl = []\n",
    "\n",
    "attention_weights_all_head = []\n",
    "sampling_grid_rescale_all_head = []\n",
    "\n",
    "for head_i in range(n_heads):\n",
    "    # N_, Lq_, lvl, P_, 2 -> N_, Lq_, lvl*P_, 2\n",
    "    sampling_grid_head_ = sampling_grids[:, :, head_i].flatten(2, 3)\n",
    "    # N_, Lq_, lvl, P_ -> N_, Lq_, lvl*P_\n",
    "    attention_weights_head_ = attention_weights_reshape[:, :, head_i].flatten(2, 3)\n",
    "    attention_weights_all_head.append(attention_weights_head_) # N_, Lq_, lvl*P_\n",
    "\n",
    "    sampling_grid_head_rescale = sampling_grid_head_ * torch.tensor([img_size[:, 1], img_size[:, 0]], dtype=torch.float32).to(sampling_grid_head_.device)\n",
    "    sampling_grid_rescale_all_head.append(sampling_grid_head_rescale) # N_, Lq_, lvl*P_, 2\n",
    "\n",
    "for lid_, (H_, W_) in enumerate(value_spatial_shapes):\n",
    "    # N_, Lq_, M_, P_, 2 -> N_, Lq_, M_*P_, 2\n",
    "    sampling_grid_l_ = sampling_grids[:, :, :, lid_].flatten(2, 3)\n",
    "    # N_, Lq_, M_, P_ -> N_, Lq_, M_*P_\n",
    "    attention_weights_l_ = attention_weights_reshape[:, :, :, lid_].flatten(2, 3)\n",
    "    attention_weights_all_lvl.append(attention_weights_l_) # N_, Lq_, M_*P_\n",
    "\n",
    "    sampling_grid_l_rescale = sampling_grid_l_ * torch.tensor([img_size[:, 1], img_size[:, 0]], dtype=torch.float32).to(sampling_grid_l_.device)\n",
    "    sampling_grid_rescale_all_lvl.append(sampling_grid_l_rescale) # N_, Lq_, M_*P_, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_obj_logits = outputs['pred_obj_logits'] # N_, Lq_, 80\n",
    "out_verb_logits = outputs['pred_verb_logits'] # N_, Lq_, 117\n",
    "\n",
    "out_obj_class = out_obj_logits.softmax(-1)[0].max(-1).indices # Lq_\n",
    "obj_scores = out_obj_logits.softmax(-1)[0].max(-1).values # Lq_\n",
    "verb_scores = out_verb_logits.sigmoid()[0] # Lq_, 117\n",
    "\n",
    "index = 0\n",
    "for verb_score in verb_scores: \n",
    "    verb_score_max = verb_score.cpu().detach().numpy().max()\n",
    "    obj_scores[index] *= verb_score_max\n",
    "    index += 1\n",
    "thres = np.sort(obj_scores.detach().cpu().numpy())[::-1][1]\n",
    "keep = obj_scores > thres\n",
    "keep_num =torch.nonzero(keep).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sub_boxes = outputs['pred_sub_boxes'] # N_, Lq_, 4\n",
    "out_obj_boxes = outputs['pred_obj_boxes'] # N_, Lq_, 4\n",
    "out_verb_boxes = verb_boxes_list[0] # N_, Lq_, 4\n",
    "im_size = (img_size[:, 1], img_size[:, 0])\n",
    "\n",
    "sub_box_priors = model.refpoint_sub_embed.weight.sigmoid()  # Lq_, 4\n",
    "\n",
    "obj_box_priors = model.refpoint_obj_embed.weight.sigmoid()\n",
    "\n",
    "sub_box_priors = rescale_bboxes(sub_box_priors[keep].cpu(), im_size)\n",
    "obj_box_priors = rescale_bboxes(obj_box_priors[keep].cpu(), im_size)\n",
    "\n",
    "sub_boxes = rescale_bboxes(out_sub_boxes[0, keep].cpu(), im_size)\n",
    "obj_boxes = rescale_bboxes(out_obj_boxes[0, keep].cpu(), im_size)\n",
    "verb_boxes = rescale_bboxes(out_verb_boxes[0, keep].cpu(), im_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display result\n",
    "\n",
    "Now let's visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJSUB_BOX_WIDTH = 7\n",
    "POINT_SIZE = 9\n",
    "POINT_ALPHA = 0.8\n",
    "VERB_BOX_WIDTH = 6\n",
    "RGB_POINT = False\n",
    "output_dir = './'\n",
    "# get the feature map shape\n",
    "\n",
    "img_cv = cv2.imread(iamge_path)\n",
    "height, width, _ = img_cv.shape\n",
    "\n",
    "imgs_obj = []\n",
    "# for idx, (sx1, sy1, sx2, sy2), (ox1, oy1, ox2, oy2), (vx1, vy1, vx2, vy2),(vxsM1, vysM1, vxsM2, vysM2),(vxM1, vyM1, vxM2, vyM2) in zip(keep.nonzero(), sub_boxes, obj_boxes, verb_boxes, verb_boxes_sMBR, verb_boxes_MBR):\n",
    "for idx, (sx1, sy1, sx2, sy2), (ox1, oy1, ox2, oy2), (vx1, vy1, vx2, vy2) in zip(keep.nonzero(), sub_boxes, obj_boxes, verb_boxes):\n",
    "    img_copy = img_cv.copy()\n",
    "\n",
    "    # resize img to 2 times larger\n",
    "    img_copy = cv2.resize(img_copy, (width*2, height*2))\n",
    "    \n",
    "    imgs_obj.append(img_copy)\n",
    "\n",
    "\n",
    "# save anchor box priors\n",
    "img_prior = imgs_obj[0].copy()\n",
    "for (sx1_prior, sy1_prior, sx2_prior, sy2_prior), (ox1_prior, oy1_prior, ox2_prior, oy2_prior) in zip(sub_box_priors, obj_box_priors):\n",
    "    cv2.rectangle(img_prior, (int(sx1_prior)*2, int(sy1_prior)*2), (int(sx2_prior)*2, int(sy2_prior)*2), (0,220,0), OBJSUB_BOX_WIDTH)\n",
    "    cv2.rectangle(img_prior, (int(ox1_prior)*2, int(oy1_prior)*2), (int(ox2_prior)*2, int(oy2_prior)*2), (0,0,220), OBJSUB_BOX_WIDTH)\n",
    "cv2.imwrite('{}/{}_prior.jpg'.format(output_dir, iamge_path.split('/')[-1][:-4]), img_prior)\n",
    "\n",
    "\n",
    "img_i2 = imgs_obj[0].copy()\n",
    "img_i2_base = imgs_obj[0].copy()\n",
    "point_num = n_levels * n_points\n",
    "point_all = 0\n",
    "for head_idx in range(n_heads):\n",
    "    sampling_grid_head_i = sampling_grid_rescale_all_head[head_idx][0, keep].cpu()[0]\n",
    "    attention_weights_head_i = attention_weights_all_head[head_idx][0, keep].cpu()[0]\n",
    "\n",
    "    # change the order of the attention weights from low to high\n",
    "    order = torch.argsort(attention_weights_head_i)\n",
    "    attention_weights_head_i = attention_weights_head_i[order]\n",
    "    sampling_grid_head_i = sampling_grid_head_i[order]\n",
    "\n",
    "    attention_weights_head_min = attention_weights_head_i.min()\n",
    "    attention_weights_head_max = attention_weights_head_i.max()\n",
    "    gap = attention_weights_head_max - attention_weights_head_min\n",
    "    for p_idx in range(point_num):\n",
    "        sample_grid_head_i, attn_weight_head_i = sampling_grid_head_i[p_idx], attention_weights_head_i[p_idx]\n",
    "        attn_weight_head_i = attn_weight_head_i - attention_weights_head_min\n",
    "        x, y = sample_grid_head_i\n",
    "        if RGB_POINT:\n",
    "            color = getJetColorRGB(attn_weight_head_i/gap, 0, 1)\n",
    "        else:\n",
    "            color = getJetColorRB(attn_weight_head_i/gap, 0, 1)\n",
    "        if color[0]< 256*0.6:\n",
    "            cv2.circle(img_i2, (int(x)*2,int(y)*2), POINT_SIZE, color, -1)\n",
    "            point_all = point_all + 1\n",
    "        else:\n",
    "            cv2.circle(img_i2, (int(x)*2,int(y)*2), POINT_SIZE, np.array([114.0,114.0,114.0]), -1)\n",
    "\n",
    "    \n",
    "result = cv2.addWeighted(img_i2, POINT_ALPHA, img_i2_base, 1-POINT_ALPHA, 0)\n",
    "for idx, (sx1, sy1, sx2, sy2), (ox1, oy1, ox2, oy2), (vx1, vy1, vx2, vy2) in zip(keep.nonzero(), sub_boxes, obj_boxes, verb_boxes):\n",
    "    cv2.rectangle(result, (int(sx1)*2,int(sy1)*2), (int(sx2)*2,int(sy2)*2), (0,220,0), OBJSUB_BOX_WIDTH)\n",
    "    cv2.rectangle(result, (int(ox1)*2,int(oy1)*2), (int(ox2)*2,int(oy2)*2), (0,0,220), OBJSUB_BOX_WIDTH)\n",
    "    cv2.rectangle(result, (int(vx1)*2,int(vy1)*2), (int(vx2)*2,int(vy2)*2), (160,48,112), VERB_BOX_WIDTH)\n",
    "cv2.imwrite('{}/{}_attn_all.jpg'.format(output_dir, iamge_path.split('/')[-1][:-4]), result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
